# 6 순환 신경망(recurrent neural network, rnn)
## 6.1 기본 순환 신경망 vanilla recurrent neural network, rnn
### 6.1.1 rnn(recurrent neural network) 소개
#### cnn vs rnn
x -> y = f(x;θ) -> y  
x -> h_t = f(x_t, h_t-1;θ) -> y

#### 각 구조가 적용되는 데이터
cnn 데이터의 예  
표형태의 데이터(tabular data), 영상(image)

rnn 데이터의 예  
연속 데이터(sequential data), 시계열 데이터(time-series data)

#### 연속 데이터(sequential data) vs 시계열 데이터(time-series data)  
타임 스탬프(time-stamp) 유무에 따른 차이

#### 연속 데이터(sequential data)  
데이터의 순서 정보가 중요  
예) 텍스트 문장 : 단어의 순서  
예) (샘플링 주기가 일정)

#### 시계열 데이터(time-series data)  
연속 데이터의 일종  
데이터가 발생한 시각 정보가 중요  
예) 주식 : 가격의 순서와 발생 시점  
예) 센서 데이터

#### rnn 공부 팁  
시간(순서) 개념이 추가되어 체감 난이도 상승  
생성 문제(generation task)가 아니면 시간 개념이 구현에 들어갈 필요 없음  
**입/출력 데이터의 모양(shape)**만 알아도 많이 이해한 것

### 6.1.2 rnn 구조
h_t = f(x_t, h_t-1;θ)

#### 입력 텐서(input tensor)
|x_t| = (batch_size, 1, input_size)  
|X| = (batch_size, n, input_size), where X={x_1, x_2, ..., x_n}

#### 출력 텐서(output tensor)
|h_t| = (batch_size, hidden_size)  
|h_1:n| = (batch_size, n, hidden_size), where h_1:n={h_1, h_2, ..., h_n} -> y

#### multi-layered rnn
#### 출력 텐서(output tensor)
|h_1:n| = (batch_size, n, hidden_size)
#### 은닉 상태 텐서(hidden state tensor)
|h_1| = (# of layers, batch_size, hidden_size)

### 6.1.3 rnn 활용 사례


### 6.1.4 rnn 학습 : back-propagation through time(BPTT)


## 6.2 발전된 순환 신경망 advanced rnn


## 6.3 순환 신경망 기반 자연어 생성 rnn-based natural language generation

