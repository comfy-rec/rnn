# 6 순환 신경망(recurrent neural network, rnn)
## 6.1 기본 순환 신경망 vanilla recurrent neural network, rnn
### 6.1.1 rnn(recurrent neural network) 소개
#### cnn vs rnn
x -> y = f(x;θ) -> y  
x -> h_t = f(x_t, h_t-1;θ) -> y

#### 각 구조가 적용되는 데이터
cnn 데이터의 예  
표형태의 데이터(tabular data), 영상(image)

rnn 데이터의 예  
연속 데이터(sequential data), 시계열 데이터(time-series data)

#### 연속 데이터(sequential data) vs 시계열 데이터(time-series data)  
타임 스탬프(time-stamp) 유무에 따른 차이

#### 연속 데이터(sequential data)  
데이터의 순서 정보가 중요  
예) 텍스트 문장 : 단어의 순서  
예) 기계 번역 (샘플링 주기가 일정)

#### 시계열 데이터(time-series data)  
연속 데이터의 일종  
데이터가 발생한 시각 정보가 중요  
예) 주식 : 가격의 순서와 발생 시점  
예) 센서 데이터

#### rnn 공부 팁  
시간(순서) 개념이 추가되어 체감 난이도 상승  
생성 문제(generation task)가 아니면 시간 개념이 구현에 들어갈 필요 없음  
**입/출력 데이터의 모양(shape)** 만 알아도 많이 이해한 것

### 6.1.2 rnn 구조
h_t = f(x_t, h_t-1;θ)

#### 입력 텐서(input tensor)
|x_t| = (batch_size, 1, input_size)  
|X| = (batch_size, n, input_size), where X={x_1, x_2, ..., x_n}

#### 출력 텐서(output tensor)
|h_t| = (batch_size, hidden_size)  
|h_1:n| = (batch_size, n, hidden_size), where h_1:n={h_1, h_2, ..., h_n} -> y

#### multi-layered rnn
#### 출력 텐서(output tensor)
|h_1:n| = (batch_size, n, hidden_size)
#### 은닉 상태 텐서(hidden state tensor)
|h_t| = (# of layers, batch_size, hidden_size)
#### 입력 텐서(input tensor) 
|X| = (batch_size, n, input_size)

#### bidirectional multi-layered rnn
#### 출력 텐서(output tensor)
|h_1:n| = (batch_size, n, hidden_size x # of directions)
#### 은닉 상태 텐서(hidden state tensor)
|h_t| = (# of directions x # of layers, batch_size, hidden_size)

### rnn 구조 요약
#### 기본 rnn 에서 은닉 상태(hidden state) 는 곧 출력
#### multi-layered rnn
출력은 마지막 레이어의 모든 시간 스텝(time-step)의 은닉 상태  
은닉 상태는 마지막 시간 스텝의 모든 레이어의 은닉 상태
#### bi-directional rnn
출력은 은닉 상태의 두 배  
은닉 상태는 레이어 개수의 두 배

### 6.1.3 rnn 활용 사례

다대일(many to one) : 텍스트 분류(text classification)  
일대다(one to many) : 자연어 생성(nlg), 기계 번역(machine translation)  
다대다(many to many) : mrc, 문법 태깅(pos tagging)

두 가지 접근법
1) non-autoregressive (non-generative)
현재 상태가 앞, 뒤 상태를 통해서 결정되는 경우
예) 품사 태깅(pos tagging, part of speech), 텍스트 분류(text classification)
bidirectional rnn 사용 (권장)

2) autoregressive (generative)
현재 상태가 과거의 상태에 의존해서 결정되는 경우
예) 자연어 생성(nlg), 기계 번역(machine translation)
일대다(one-to-many) 경우에 해당
bidirectional rnn 사용 불가

#### 1) 다대다(many to many)
예: 품사 태깅(pos tagging)
#### 2) 다대일(many to one)
예 : 텍스트 분류(text classification)  
#### 3) 일대다(one to many)
예: 자연어 생성(natural language generation, nlg)
sequence-to-sequence
예) 기계 번역(machine translation)  

학습

### 6.1.4 rnn 학습 : back-propagation through time(BPTT)

## 6.2 발전된 순환 신경망 advanced rnn


## 6.3 순환 신경망 기반 자연어 생성 rnn-based natural language generation

