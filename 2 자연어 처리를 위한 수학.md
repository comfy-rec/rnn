# 2 자연어 처리를 위한 수학 (mathematics for natural language process)
"young man, in mathematics you don't understand things. you just get used to them."  
felix smith 라는 학생이 "수학적인 특성이 이해가 되지않아 걱정이다." 라 하자, 폰 노이만의 대답  
"young man, with john von neumann you don't interpret quotes, you just get used to them." (meni rosenfeld)

## 2.1 확률의 기초 probability

### 2.1.1 확률 변수 random variable
#### 예 : 두 개의 동전 던지기
각각의 동전은 앞면 혹은 뒷면이 나옴  
앞면을 Head(H), 뒷면을 Tail(T) 라고 하면 다음처럼 나타낼 수 있음  
Ω = {HH, HT, TH, TT}  
이는 동전 두 개를 던지는 사건에서 발생할 수 있는 모든 경우임. 이를 표본 공간이라(sample space) 부름

두 동전이 모두 앞면으로 나오는 확률은 얼마일까?  
P(X=사건) = P(X=2) = 1/4  
여기서 X 를 확률 변수라고(random variable) 부름  
확률 변수는 어떤 사건을 실수 표현으로 매핑하는 일종의 함수.  
여기서는 확률 변수를 '앞면이 나오는 횟수' 로 보고 동전 두 개를 던져 앞면이 나오는 사상 HH 를 숫자 2로 매핑함  
P(X=사건) = 확률, ∑_(i=1)^(N)P(X=x_i) = 1 (은 모든 사건의 개수)

#### 용어 사용 예
시행 experiment : 두 개의 동전 던지기  
사상 outcomes : HH, HT, TH, TT  
표본 공간 sample space : {HH, HT, TH, TT}  
사건 event  
(예) '두 동전이 모두 앞면으로 나오는 경우' {HH}  
(예) '적어도 한 개 동전이 앞면으로 나오는 경우' {HH, HT, TH}  
확률 변수 random variable  
(예) 사상 중 HH -> 숫자 2로 매핑  
(예) 사상 중 HH -> 숫자 2로 매핑, HT & TH -> 숫자 1로 매핑

#### 머신러닝에서 확률 변수 사용 예
사람의 속성 표현  
머신러닝에서는 데이터를 일반적으로 확률 변수라 간주함  
X^T = [키, 몸무게, 나이], x_1^T = [165, 50, 20], x_2^T = [178, 65, 30]  
예) 이미지의 픽셀값, 문장의 단어 벡터

### 2.1.2 확률 변수와 확률 분포
#### 1) 이산 확률 변수와 연속 확률 변수
이산적인 사건과 연속적인 사건을 표현하기 위해, 확률 변수에는 이산 확률 변수와 연속 확률 변수라는 개념이 존재함  
이산 확률 변수 discrete random variable  
확률 변수 X가 취할 수 있는 값들이 이산적으로 셀 수 있는 경우  
예) 공장에서 발생하는 불량품의 개수, 올 한해 발생한 교통사고 횟수, 동전 하나 던질 때 앞면이 나오는 횟수  
연속 확률 변후 continuous random variable  
확률 변수 X가 취할 수 있는 값들이 어떤 범위로 주어지는 경우  
예 사람의 키, 체중, 나이 등  
확률 분포 probability distribution  
확률 변수가 특정한 값을 가질 확률을 나타내는 함수

#### 2) 이산 확률 분포

#### 3) 연속 확률 분포

### 2.1.3 이항분포, 다항분포, 정규분포
#### 1) 이항 분포 binomial/bernoulli distribution

#### 2) 다항 분포 multinomial/categorical distribution

#### 3) 정규 분포normal distribution

### 2.1.4 조건부 확률과 베이즈 정리
#### 1) 조건부 확률 conditional probability

#### 2) 베이즈 정리

### 2.1.5 기댓값과 분산
#### 1) 기댓값 expectation, E(X)

#### 2) 분산 variation V(X)

## 2.2 최대우도추정과 최대사후확률추정 mle & map
### 2.2.1 MLE
### 2.2.2 MAP

## 2.3 정보이론과 엔트로피 information theory & entropy
### 2.3.1 정보량
### 2.3.2 엔트로피
### 2.3.3 KL-Divergence, Preplexity
